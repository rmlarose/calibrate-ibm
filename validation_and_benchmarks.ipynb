{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46e09bce-4cfd-4e6b-8a8d-0b85a7d9647a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import pickle\n",
    "from openfermion import qubit_operator_to_pauli_sum\n",
    "\n",
    "from kcommute.sorted_insertion import get_si_sets, get_si_sets_v2, get_terms_ordered_by_abscoeff\n",
    "from kcommute.sorted_insertion_v2_parallel import get_si_sets_v2_parallel\n",
    "from Hamiltonians.load_h2o import load_h2o_hamiltonian\n",
    "from Hamiltonians.load_owp import load_owp_hamiltonian"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272be967-7d9a-4274-a7f8-b1d85c1a2843",
   "metadata": {},
   "source": [
    "This notebook assesses the validity and timing benchmarks of a pair of refactors which modify get_si_sets from the original kcommute repository: https://github.com/rmlarose/kcommute/tree/main/kcommute .  \n",
    "\n",
    "At the end of this notebook it shows how one would use this code to compute the groupings for the OWP Hamiltonian as required for future hardware work: https://github.com/rmlarose/calibrate-ibm .  To perform this optimally, please request 17 cores and sufficient memory, else switch to the single core serial method (roughly 3x more time) which has been commented out.\n",
    "\n",
    "The serial method get_si_sets_v2 benefits from utilizing the symplectic form representation.  This enables relevant operations to be implemented as simple bitwise operations and parity checks.  Some additional optimizations were made based on the cheapness of 1-commuting checks in this formalism, as well as using a 1-commuting representative for each group.  \n",
    "\n",
    "The parallel method get_si_sets_v2_parallel utilizes the same data structure and methods but depends on several further optimizations to squeeze out another ~3x speedup (heavily dependent on number of cores and parameters.  Suggested parameters chosen at the end of the notebook).  Parallel workers check for incompatibility with existing groups.  Main process maintains authority over the grouping and sorted insertion ordering is maintained.  Workers operate in a 'leapfrog' fashion checking whether strings in their chunk are incompatible with existing groupings.  Existing groupings and incompatibilities are handshaked from the parent process intermittently.  Tuning the chunk size and frequency is non-trivial and exhibits a non-monotonic tradeoff between information movement and structure awareness of child processes.  Data is passed in a minimal way to allow all processes to update their knowledge while minimizing overhead.  This method has been validated on a small variety of systems and seems to perform better for systems of the size of OWP.  Obtaining optimal parallel scaling seems a difficult challenge while maintaining the sorted insertion ordering.  By relaxing the ordering one could enable batching which would enable more efficient parallelism.  This is a future work if needed.  It would seem this method could reasonably scale to 100 qubit chemical systems in runs that would take on the order of weeks.  For OWP, I anticipate on the order of hours (50k terms takes about 30 seconds with 17 cores, so roughly 8 hours for 500k terms in worst case).  \n",
    "\n",
    "The preceding validations and benchmarks take around 10 minutes to run provided the Hamiltonians are cached.  (Caching simply skips the loading, JWT, and random subset selection for the OWP 50k validation.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "test_k_commuting",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded H2O Hamiltonian: 14 qubits, 1620 terms\n",
      "Testing k=1\n",
      "Results:\n",
      "  get_si_sets:             443 commuting groups\n",
      "  get_si_sets_v2:          443 commuting groups\n",
      "  get_si_sets_v2_parallel: 443 commuting groups\n",
      "\n",
      "✓ PASS: All three implementations produce identical results for k=1\n",
      "Testing k=4\n",
      "\n",
      "Results:\n",
      "  get_si_sets:             250 commuting groups\n",
      "  get_si_sets_v2:          250 commuting groups\n",
      "  get_si_sets_v2_parallel: 250 commuting groups\n",
      "\n",
      "✓ PASS: All three implementations produce identical results for k=4\n",
      "Testing k=N\n",
      "Blocks: [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]]\n",
      "\n",
      "Results:\n",
      "  get_si_sets:             65 commuting groups\n",
      "  get_si_sets_v2:          65 commuting groups\n",
      "  get_si_sets_v2_parallel: 65 commuting groups\n",
      "\n",
      "✓ PASS: All three implementations produce identical results for k=N\n"
     ]
    }
   ],
   "source": [
    "def compare_commuting_sets(sets1, sets2):\n",
    "    if len(sets1) != len(sets2):\n",
    "        print(f\"Different number of sets: {len(sets1)} vs {len(sets2)}\")\n",
    "        return False\n",
    "    \n",
    "    def sort_key(commset):\n",
    "        return (len(commset), str(sorted([str(term) for term in commset])))\n",
    "    \n",
    "    sets1_sorted = sorted(sets1, key=sort_key)\n",
    "    sets2_sorted = sorted(sets2, key=sort_key)\n",
    "    \n",
    "    for i, (set1, set2) in enumerate(zip(sets1_sorted, sets2_sorted)):\n",
    "        if len(set1) != len(set2):\n",
    "            print(f\"Set {i}: Different lengths {len(set1)} vs {len(set2)}\")\n",
    "            return False\n",
    "        \n",
    "        terms1 = sorted([str(term) for term in set1])\n",
    "        terms2 = sorted([str(term) for term in set2])\n",
    "        \n",
    "        if terms1 != terms2:\n",
    "            print(f\"Set {i}: Different terms\")\n",
    "            return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "hdf5_path = 'Hamiltonians/monomer_eqb.hdf5'\n",
    "hamiltonian, nqubits, nterms = load_h2o_hamiltonian(hdf5_path)\n",
    "\n",
    "hamiltonian_cirq = qubit_operator_to_pauli_sum(hamiltonian)\n",
    "cirq_qubits = sorted(hamiltonian_cirq.qubits)\n",
    "blocks_int = [list(range(nqubits))]\n",
    "\n",
    "terms = get_terms_ordered_by_abscoeff(hamiltonian)\n",
    "terms = [t for t in terms if () not in t.terms.keys()]\n",
    "\n",
    "print(\"Testing k=1\")\n",
    "\n",
    "blocks_cirq_k1 = [[q] for q in cirq_qubits]\n",
    "blocks_int_k1 = [[i] for i in range(nqubits)]\n",
    "\n",
    "sets1_k1 = get_si_sets(hamiltonian, blocks_cirq_k1, verbosity=0)\n",
    "sets2_k1 = get_si_sets_v2(hamiltonian, blocks_int_k1, verbosity=0)\n",
    "sets3_k1 = get_si_sets_v2_parallel(terms, blocks_int_k1, verbosity=0, num_workers=2)\n",
    "\n",
    "print(f\"Results:\")\n",
    "print(f\"  get_si_sets:             {len(sets1_k1)} commuting groups\")\n",
    "print(f\"  get_si_sets_v2:          {len(sets2_k1)} commuting groups\")\n",
    "print(f\"  get_si_sets_v2_parallel: {len(sets3_k1)} commuting groups\")\n",
    "\n",
    "if compare_commuting_sets(sets1_k1, sets2_k1) and compare_commuting_sets(sets1_k1, sets3_k1):\n",
    "    print(\"\\n✓ PASS: All three implementations produce identical results for k=1\")\n",
    "else:\n",
    "    print(\"\\n✗ FAIL: Implementations produce different results for k=1\")\n",
    "\n",
    "print(\"Testing k=4\")\n",
    "\n",
    "blocks_cirq_k4 = [cirq_qubits[i:i+4] for i in range(0, nqubits, 4)]\n",
    "blocks_int_k4 = [list(range(i, i+4)) for i in range(0, nqubits, 4)]\n",
    "if nqubits % 4 != 0:\n",
    "    blocks_int_k4[-1] = list(range(blocks_int_k4[-1][0], nqubits))\n",
    "    blocks_cirq_k4[-1] = cirq_qubits[blocks_cirq_k4[-1][0].x:]\n",
    "\n",
    "sets1_k4 = get_si_sets(hamiltonian, blocks_cirq_k4, verbosity=0)\n",
    "sets2_k4 = get_si_sets_v2(hamiltonian, blocks_int_k4, verbosity=0)\n",
    "sets3_k4 = get_si_sets_v2_parallel(terms, blocks_int_k4, verbosity=0, num_workers=2)\n",
    "\n",
    "print(f\"\\nResults:\")\n",
    "print(f\"  get_si_sets:             {len(sets1_k4)} commuting groups\")\n",
    "print(f\"  get_si_sets_v2:          {len(sets2_k4)} commuting groups\")\n",
    "print(f\"  get_si_sets_v2_parallel: {len(sets3_k4)} commuting groups\")\n",
    "\n",
    "if compare_commuting_sets(sets1_k4, sets2_k4) and compare_commuting_sets(sets1_k4, sets3_k4):\n",
    "    print(\"\\n✓ PASS: All three implementations produce identical results for k=4\")\n",
    "else:\n",
    "    print(\"\\n✗ FAIL: Implementations produce different results for k=4\")\n",
    "\n",
    "print(\"Testing k=N\")\n",
    "\n",
    "blocks_cirq_kN = [cirq_qubits]\n",
    "blocks_int_kN = [list(range(nqubits))]\n",
    "\n",
    "print(f\"Blocks: {blocks_int_kN}\")\n",
    "\n",
    "sets1_kN = get_si_sets(hamiltonian, blocks_cirq_kN, verbosity=0)\n",
    "sets2_kN = get_si_sets_v2(hamiltonian, blocks_int_kN, verbosity=0)\n",
    "sets3_kN = get_si_sets_v2_parallel(terms, blocks_int_kN, verbosity=0, num_workers=2)\n",
    "\n",
    "print(f\"\\nResults:\")\n",
    "print(f\"  get_si_sets:             {len(sets1_kN)} commuting groups\")\n",
    "print(f\"  get_si_sets_v2:          {len(sets2_kN)} commuting groups\")\n",
    "print(f\"  get_si_sets_v2_parallel: {len(sets3_kN)} commuting groups\")\n",
    "\n",
    "if compare_commuting_sets(sets1_kN, sets2_kN) and compare_commuting_sets(sets1_kN, sets3_kN):\n",
    "    print(\"\\n✓ PASS: All three implementations produce identical results for k=N\")\n",
    "else:\n",
    "    print(\"\\n✗ FAIL: Implementations produce different results for k=N\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "test_h2o",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing H2O Hamiltonian with all three implementations\n",
      "Loaded H2O Hamiltonian: 14 qubits, 1620 terms\n",
      "\n",
      "Running get_si_sets...\n",
      "Running get_si_sets_v2...\n",
      "Running get_si_sets_v2_parallel...\n",
      "\n",
      "Results:\n",
      "  get_si_sets:             65 groups\n",
      "  get_si_sets_v2:          65 groups\n",
      "  get_si_sets_v2_parallel: 65 groups\n",
      "\n",
      "✓ PASS: All three implementations produce 65 groups\n",
      "H2O Hamiltonian Timing Comparison\n",
      "\n",
      "Running get_si_sets...\n",
      "  Time: 58.241s, 65 groups\n",
      "\n",
      "Running get_si_sets_v2...\n",
      "  Time: 0.076s, 65 groups, speedup: 770.98x\n",
      "\n",
      "Running get_si_sets_v2_parallel...\n",
      "  Time: 0.317s, 65 groups, speedup: 183.51x\n",
      "Summary:\n",
      "  get_si_sets:             58.241s, 65 groups (baseline)\n",
      "  get_si_sets_v2:          0.076s, 65 groups (770.98x faster)\n",
      "  get_si_sets_v2_parallel: 0.317s, 65 groups (183.51x faster)\n",
      "\n",
      "✓ PASS: All three implementations produce 65 groups\n"
     ]
    }
   ],
   "source": [
    "print(\"Testing H2O Hamiltonian with all three implementations\")\n",
    "\n",
    "hdf5_path = 'Hamiltonians/monomer_eqb.hdf5'\n",
    "hamiltonian, nqubits, nterms = load_h2o_hamiltonian(hdf5_path)\n",
    "\n",
    "hamiltonian_cirq = qubit_operator_to_pauli_sum(hamiltonian)\n",
    "cirq_qubits = sorted(hamiltonian_cirq.qubits)\n",
    "blocks = [cirq_qubits]\n",
    "blocks_int = [list(range(nqubits))]\n",
    "\n",
    "print(f\"\\nRunning get_si_sets...\")\n",
    "groups_v1 = get_si_sets(hamiltonian, blocks, verbosity=0)\n",
    "\n",
    "print(f\"Running get_si_sets_v2...\")\n",
    "groups_v2 = get_si_sets_v2(hamiltonian, blocks_int, verbosity=0)\n",
    "\n",
    "print(f\"Running get_si_sets_v2_parallel...\")\n",
    "terms = get_terms_ordered_by_abscoeff(hamiltonian)\n",
    "terms = [t for t in terms if () not in t.terms.keys()]\n",
    "groups_v2_parallel = get_si_sets_v2_parallel(terms, blocks_int, verbosity=0, num_workers=2)\n",
    "\n",
    "print(f\"\\nResults:\")\n",
    "print(f\"  get_si_sets:             {len(groups_v1)} groups\")\n",
    "print(f\"  get_si_sets_v2:          {len(groups_v2)} groups\")\n",
    "print(f\"  get_si_sets_v2_parallel: {len(groups_v2_parallel)} groups\")\n",
    "\n",
    "if len(groups_v1) == len(groups_v2) == len(groups_v2_parallel):\n",
    "    print(f\"\\n✓ PASS: All three implementations produce {len(groups_v1)} groups\")\n",
    "else:\n",
    "    print(f\"\\n✗ FAIL: Implementations produce different numbers of groups\")\n",
    "\n",
    "print(\"H2O Hamiltonian Timing Comparison\")\n",
    "\n",
    "print(f\"\\nRunning get_si_sets...\")\n",
    "start = time.time()\n",
    "groups_v1 = get_si_sets(hamiltonian, blocks, verbosity=0)\n",
    "t_v1 = time.time() - start\n",
    "print(f\"  Time: {t_v1:.3f}s, {len(groups_v1)} groups\")\n",
    "\n",
    "print(f\"\\nRunning get_si_sets_v2...\")\n",
    "start = time.time()\n",
    "groups_v2 = get_si_sets_v2(hamiltonian, blocks_int, verbosity=0)\n",
    "t_v2 = time.time() - start\n",
    "speedup_v2 = t_v1 / t_v2\n",
    "print(f\"  Time: {t_v2:.3f}s, {len(groups_v2)} groups, speedup: {speedup_v2:.2f}x\")\n",
    "\n",
    "print(f\"\\nRunning get_si_sets_v2_parallel...\")\n",
    "start = time.time()\n",
    "groups_v2_parallel = get_si_sets_v2_parallel(terms, blocks_int, verbosity=0, num_workers=2)\n",
    "t_v2_parallel = time.time() - start\n",
    "speedup_v2_parallel = t_v1 / t_v2_parallel\n",
    "print(f\"  Time: {t_v2_parallel:.3f}s, {len(groups_v2_parallel)} groups, speedup: {speedup_v2_parallel:.2f}x\")\n",
    "\n",
    "print(f\"Summary:\")\n",
    "print(f\"  get_si_sets:             {t_v1:.3f}s, {len(groups_v1)} groups (baseline)\")\n",
    "print(f\"  get_si_sets_v2:          {t_v2:.3f}s, {len(groups_v2)} groups ({speedup_v2:.2f}x faster)\")\n",
    "print(f\"  get_si_sets_v2_parallel: {t_v2_parallel:.3f}s, {len(groups_v2_parallel)} groups ({speedup_v2_parallel:.2f}x faster)\")\n",
    "\n",
    "if len(groups_v1) == len(groups_v2) == len(groups_v2_parallel):\n",
    "    print(f\"\\n✓ PASS: All three implementations produce {len(groups_v1)} groups\")\n",
    "else:\n",
    "    print(f\"\\n✗ FAIL: Implementations produce different numbers of groups\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5811b4a7-0ed2-4a6d-b814-b167e90bc812",
   "metadata": {},
   "source": [
    "Here we see the parallel version slightly underperform.  At 0.76 seconds for the get_si_sets_v2, parallel overhead simply dominates any possible advantage.  As problems become harder, parallel advantage begins to emerge:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "test_owp_50k",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing OWP 50k terms subset - Correctness\n",
      "Testing 50000 terms\n",
      "\n",
      "Running get_si_sets_v2...\n",
      "Running get_si_sets_v2_parallel...\n",
      "\n",
      "Results:\n",
      "  get_si_sets_v2:          1189 groups\n",
      "  get_si_sets_v2_parallel: 1189 groups\n",
      "\n",
      "✓ PASS: Both implementations produce 1189 groups\n",
      "OWP 50k terms subset - Timing\n",
      "Testing 50000 terms\n",
      "\n",
      "Running get_si_sets_v2...\n",
      "  Time: 75.854s, 1189 groups\n",
      "\n",
      "Running get_si_sets_v2_parallel...\n",
      "  Time: 53.993s, 1189 groups, speedup: 1.40x\n",
      "Summary:\n",
      "  get_si_sets_v2:          75.854s, 1189 groups (baseline)\n",
      "  get_si_sets_v2_parallel: 53.993s, 1189 groups (1.40x faster)\n",
      "\n",
      "✓ PASS: Both implementations produce 1189 groups\n"
     ]
    }
   ],
   "source": [
    "print('Testing OWP 50k terms subset - Correctness')\n",
    "\n",
    "with open('tests/cached_50k_subset.pkl', 'rb') as f:\n",
    "    P, nq = pickle.load(f)\n",
    "\n",
    "terms = get_terms_ordered_by_abscoeff(P)\n",
    "terms = [t for t in terms if () not in t.terms.keys()]\n",
    "blocks = [list(range(nq))]\n",
    "\n",
    "print(f'Testing {len(terms)} terms\\n')\n",
    "\n",
    "print('Running get_si_sets_v2...')\n",
    "g_serial = get_si_sets_v2(P, blocks, verbosity=0)\n",
    "\n",
    "print('Running get_si_sets_v2_parallel...')\n",
    "g_parallel = get_si_sets_v2_parallel(terms, blocks, verbosity=0, num_workers=3, handshake_interval=75)\n",
    "\n",
    "print(f'\\nResults:')\n",
    "print(f'  get_si_sets_v2:          {len(g_serial)} groups')\n",
    "print(f'  get_si_sets_v2_parallel: {len(g_parallel)} groups')\n",
    "\n",
    "if len(g_serial) == len(g_parallel):\n",
    "    print(f'\\n✓ PASS: Both implementations produce {len(g_serial)} groups')\n",
    "else:\n",
    "    print(f'\\n✗ FAIL: Implementations produce different numbers of groups')\n",
    "\n",
    "print('OWP 50k terms subset - Timing')\n",
    "\n",
    "print(f'Testing {len(terms)} terms\\n')\n",
    "\n",
    "print('Running get_si_sets_v2...')\n",
    "start = time.time()\n",
    "g_serial = get_si_sets_v2(P, blocks, verbosity=0)\n",
    "t_serial = time.time() - start\n",
    "print(f'  Time: {t_serial:.3f}s, {len(g_serial)} groups')\n",
    "\n",
    "print('\\nRunning get_si_sets_v2_parallel...')\n",
    "start = time.time()\n",
    "g_parallel = get_si_sets_v2_parallel(terms, blocks, verbosity=0, num_workers=3, handshake_interval=75)\n",
    "t_parallel = time.time() - start\n",
    "speedup = t_serial / t_parallel\n",
    "print(f'  Time: {t_parallel:.3f}s, {len(g_parallel)} groups, speedup: {speedup:.2f}x')\n",
    "\n",
    "print(f'Summary:')\n",
    "print(f'  get_si_sets_v2:          {t_serial:.3f}s, {len(g_serial)} groups (baseline)')\n",
    "print(f'  get_si_sets_v2_parallel: {t_parallel:.3f}s, {len(g_parallel)} groups ({speedup:.2f}x faster)')\n",
    "\n",
    "\n",
    "if len(g_serial) == len(g_parallel):\n",
    "    print(f'\\n✓ PASS: Both implementations produce {len(g_serial)} groups')\n",
    "else:\n",
    "    print(f'\\n✗ FAIL: Implementations produce different numbers of groups')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d436affa-ceb4-4dda-be2e-ea863473ed2f",
   "metadata": {},
   "source": [
    "vvv Don't forget to save the grouping! vvv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "full_owp",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full OWP Hamiltonian\n",
      "Loaded OWP Hamiltonian:\n",
      "  Core energy: -479.76304624866236\n",
      "  Number of orbitals: 22\n",
      "  Number of electrons: 32\n",
      "  H1 shape: (22, 22)\n",
      "  H2 shape: (22, 22, 22, 22)\n",
      "  FermionOperator terms: 937977\n",
      "Applying Jordan-Wigner transformation...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFull OWP Hamiltonian\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m npz_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHamiltonians/owp_reactant.npz\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 4\u001b[0m hamiltonian_owp, nqubits_owp, nterms_owp \u001b[38;5;241m=\u001b[39m \u001b[43mload_owp_hamiltonian\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnpz_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m terms_owp \u001b[38;5;241m=\u001b[39m get_terms_ordered_by_abscoeff(hamiltonian_owp)\n\u001b[1;32m      7\u001b[0m terms_owp \u001b[38;5;241m=\u001b[39m [t \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m terms_owp \u001b[38;5;28;01mif\u001b[39;00m () \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m t\u001b[38;5;241m.\u001b[39mterms\u001b[38;5;241m.\u001b[39mkeys()]\n",
      "File \u001b[0;32m/mnt/ffs24/home/rowlan91/kcommute/Hamiltonians/load_owp.py:59\u001b[0m, in \u001b[0;36mload_owp_hamiltonian\u001b[0;34m(npz_path)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# Jordan-Wigner transformation\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mApplying Jordan-Wigner transformation...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 59\u001b[0m hamiltonian_qubit \u001b[38;5;241m=\u001b[39m \u001b[43mjordan_wigner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhamiltonian_ferm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m nterms \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(hamiltonian_qubit\u001b[38;5;241m.\u001b[39mterms)\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# Determine number of qubits (should be 2 * NORB = 44)\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/openfermion/transforms/opconversions/jordan_wigner.py:43\u001b[0m, in \u001b[0;36mjordan_wigner\u001b[0;34m(operator)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Apply the Jordan-Wigner transform to a FermionOperator,\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;124;03mInteractionOperator, or DiagonalCoulombHamiltonian to convert\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;124;03mto a QubitOperator.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;124;03m        DiagonalCoulombHamiltonian, or InteractionOperator.\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(operator, FermionOperator):\n\u001b[0;32m---> 43\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_jordan_wigner_fermion_operator\u001b[49m\u001b[43m(\u001b[49m\u001b[43moperator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(operator, MajoranaOperator):\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _jordan_wigner_majorana_operator(operator)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/openfermion/transforms/opconversions/jordan_wigner.py:79\u001b[0m, in \u001b[0;36m_jordan_wigner_fermion_operator\u001b[0;34m(operator)\u001b[0m\n\u001b[1;32m     75\u001b[0m                 pauli_y_component \u001b[38;5;241m=\u001b[39m QubitOperator(\n\u001b[1;32m     76\u001b[0m                     z_factors \u001b[38;5;241m+\u001b[39m ((ladder_operator[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mY\u001b[39m\u001b[38;5;124m'\u001b[39m),), \u001b[38;5;241m0.5\u001b[39mj\n\u001b[1;32m     77\u001b[0m                 )\n\u001b[1;32m     78\u001b[0m             lookup_ladder_terms[ladder_operator] \u001b[38;5;241m=\u001b[39m pauli_x_component \u001b[38;5;241m+\u001b[39m pauli_y_component\n\u001b[0;32m---> 79\u001b[0m         transformed_term \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m lookup_ladder_terms[ladder_operator]\n\u001b[1;32m     80\u001b[0m     transformed_operator \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m transformed_term\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m transformed_operator\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/openfermion/ops/operators/symbolic_operator.py:382\u001b[0m, in \u001b[0;36mSymbolicOperator.__imul__\u001b[0;34m(self, multiplier)\u001b[0m\n\u001b[1;32m    379\u001b[0m new_coefficient \u001b[38;5;241m=\u001b[39m left_coefficient \u001b[38;5;241m*\u001b[39m right_coefficient\n\u001b[1;32m    380\u001b[0m new_term \u001b[38;5;241m=\u001b[39m left_term \u001b[38;5;241m+\u001b[39m right_term\n\u001b[0;32m--> 382\u001b[0m new_coefficient, new_term \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_simplify\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    383\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnew_term\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcoefficient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_coefficient\u001b[49m\n\u001b[1;32m    384\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[38;5;66;03m# Update result dict.\u001b[39;00m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m new_term \u001b[38;5;129;01min\u001b[39;00m result_terms:\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/openfermion/ops/operators/qubit_operator.py:121\u001b[0m, in \u001b[0;36mQubitOperator._simplify\u001b[0;34m(self, term, coefficient)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m term:\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m coefficient, term\n\u001b[0;32m--> 121\u001b[0m term \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msorted\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mterm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfactor\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfactor\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    123\u001b[0m new_term \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    124\u001b[0m left_factor \u001b[38;5;241m=\u001b[39m term[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/openfermion/ops/operators/qubit_operator.py:121\u001b[0m, in \u001b[0;36mQubitOperator._simplify.<locals>.<lambda>\u001b[0;34m(factor)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m term:\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m coefficient, term\n\u001b[0;32m--> 121\u001b[0m term \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(term, key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m factor: \u001b[43mfactor\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[1;32m    123\u001b[0m new_term \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    124\u001b[0m left_factor \u001b[38;5;241m=\u001b[39m term[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print('Full OWP Hamiltonian')\n",
    "\n",
    "npz_path = 'Hamiltonians/owp_reactant.npz'\n",
    "hamiltonian_owp, nqubits_owp, nterms_owp = load_owp_hamiltonian(npz_path)\n",
    "\n",
    "terms_owp = get_terms_ordered_by_abscoeff(hamiltonian_owp)\n",
    "terms_owp = [t for t in terms_owp if () not in t.terms.keys()]\n",
    "blocks_owp = [list(range(nqubits_owp))]\n",
    "\n",
    "print(f'\\nProcessing {len(terms_owp)} terms with {nqubits_owp} qubits\\n')\n",
    "\n",
    "print('Running get_si_sets_v2_parallel with 16 workers, handshake_interval=75, chunk_divisor=2...')\n",
    "start = time.time()\n",
    "# groups_owp_serial = get_si_sets_v2(hamiltonian_owp, blocks_owp, verbosity=0)\n",
    "groups_owp = get_si_sets_v2_parallel(\n",
    "    terms_owp, \n",
    "    blocks_owp, \n",
    "    verbosity=0, \n",
    "    num_workers=16, \n",
    "    handshake_interval=75,\n",
    "    chunk_divisor=2\n",
    ")\n",
    "t_owp_parallel = time.time() - start\n",
    "\n",
    "print(f'\\nCompleted in {t_owp_parallel:.3f}s')\n",
    "print(f'Result: {len(groups_owp)} commuting groups')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
